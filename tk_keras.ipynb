{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tk.keras.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN/YTlJM7IKI5Jjf0dKQjw/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmdwn99/Computer-Vision-Study/blob/main/tk_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tf.keras.layers.Dense(units=52, activation='relu', input_shape=(13,))\n",
        "\n",
        "    * units: 해당 은닉층에서 활동하는 뉴런의 수\n",
        "\n",
        "    * activation: 활성화함수, 해당 은닉층의 가중치와 편향의 연산 결과를 어느 함수에 적합하여 출력할지 결정\n",
        "\n",
        "    * input_shape: 입력 벡터의 크기, 여기서 13은 해당 데이터 프레임의 열의 수를 나타낸다. 데이터의 구조(이미지, 영상)에 따라 달라질 수 있다. 첫 번째 은닉층에서만 정의해준다.\n",
        "\n",
        "​\n",
        "\n",
        "tf.keras.layers.Dense(units=39, activation='relu')\n",
        "\n",
        "tf.keras.layers.Dense(units=26, activation='relu')\n",
        "\n",
        "    * input_shape만 없지 나머지는 위와 동일하다. hidden layer를 연속해서 이렇게 쌓을 수 있음\n",
        "\n",
        "​\n",
        "\n",
        "tf.keras.layers.Dense(units=1)\n",
        "\n",
        "    * 마지막 output 층이다. 여기서 units가 1인 이유는 출력하는 결과가 하나이기 때문 \n",
        "\n",
        "​\n",
        "\n",
        "결론적으로 sequential 함수의 구조는 다음과 같다. \n",
        "\n",
        "\n",
        "\n",
        "첫 번째 은닉층에서 입력층이 함께 정의된다. 이후, 해결하려고 하는 문제의 복잡도에 따라 은닉층의 수와 뉴런의 수를 결정하고 마지막으로 출력층에서 1개의 유닛을 정의한다. \n",
        "\n",
        "​\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.07), loss='mse')\n",
        "\n",
        "    * compile은 모델의 최적화와 관련되는 메서드이다. 모델에게 최적화를 위해 어떤 동작을 할지를 가르쳐주는 메서드\n",
        "\n",
        "    * loss: 손실 계산 함수를 정의한다. 손실 계산 함수가 중요한 이유는 모델 결과를 판단하는 지표가 되기 때문이다. \n",
        "\n",
        "    * optimizer: 최적화 함수를 정의한다. 즉, loss를 가장 낮게하는 학습을 할 수 있도록 그 기준을 계산할 수 있는 함수를 정의한다.\n",
        "\n",
        "    * 즉, compile 메서드에서 하는 일은 loss를 최소화하는 최적화 방안을 정의하는 것이라고 할 수 있다. \n",
        "\n",
        "​\n",
        "\n",
        "model.summary()\n",
        "\n",
        "    * 생성된 모델 구조 확인\n",
        "\n",
        "​\n",
        "\n",
        "history = model.fit(train_X, train_Y, epochs=25, batch_size=32, validation_split=0.25)\n",
        "\n",
        "    * 실제 데이터에 fitting을 진행하고 진행 내역을 객체에 기록\n",
        "\n",
        "    * train_X, train_Y: 학습시킬 데이터셋 정의 \n",
        "\n",
        "    * epochs: 학습 반복 횟수\n",
        "\n",
        "    * batch_size: 학습 시 한번에 들어가는 데이터의 크기\n",
        "\n",
        "    * validation_split: train_set에서 몇 %를 검증 set으로 분리할지 결정\n",
        "\n",
        "​\n",
        "\n",
        "history.history\n",
        "\n",
        "    * history 객체의 history 메서드를 이용하면 dict 형태의 학습 기록을 확인할 수 있다. \n",
        "\n",
        "​\n",
        "\n",
        " model.evaluate(test_X, test_Y)\n",
        "\n",
        "     * 학습된 네트워크의 성능을 테스트 셋에 적용해서 확인하는 메서드\n",
        "\n",
        "​\n",
        "\n",
        "model.predict(test_X)\n",
        "\n",
        "     * test set에 대한 실제 예측값을 확인하는 메서드\n",
        "\n"
      ],
      "metadata": {
        "id": "qkoumwXNGk26"
      }
    }
  ]
}